# -*- coding: utf-8 -*-
"""COMP551 - Assignment 2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tEMOiqYb4fNBKq5F3exyrm1f9p9DRB4f

# Task 1: Acquire, preprocess, and analyze the data
"""

#%pip install ucimlrepo

"""**Key imports**"""

#from ucimlrepo import fetch_ucirepo
from google.colab import drive
import matplotlib.pyplot as plt

from sklearn.model_selection import KFold, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import auc, roc_curve, roc_auc_score, accuracy_score
from sklearn.preprocessing import label_binarize

import numpy as np
import pandas as pd
import seaborn as sns


drive.mount('/content/drive')

"""## Task 1.1 Binary Logistic Regression dataset choice

**Chose the Breast Cancer Wisconsin dataset for BLR**
"""

breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17)

pd.set_option('display.max_rows', 500)  # Show up to 500 rows
pd.set_option('display.max_columns', 100)  # Show up to 100 columns

# Get the data and display it as a well-formatted pandas DataFrame
breast_df = breast_cancer_wisconsin_diagnostic.data.features
breast_df['target'] = breast_cancer_wisconsin_diagnostic.data.targets

# Display the first 10 rows as a nice table
breast_df = breast_df.dropna()

"""## Task 1.2 Multiclass Classification dataset choice

**Chose penguin data set for multiclass classification**
"""

penguin_df = pd.read_csv("drive/MyDrive/ML_A2/penguins_size.csv").drop("island", axis=1).drop("sex", axis=1).dropna()

"""## Task 1 substasks

### 1. Read in pandas object
"""

# Display the first 10 rows as a nice table
display(breast_df.head(10))

display(penguin_df.head(10))

"""### 2. Feature importance (Binary classification)"""

# Section 2: Binary Classification Feature Importance
def compute_binary_feature_importance(x, y, feature_name):
    """
    Compute feature importance for binary classification using simple regression.
    1) Standardize X and y
    2) Compute w = (X^T y) / N
    """
    # Standardize X
    x_mean = x.mean()
    x_std = x.std() if x.std() != 0 else 1e-8
    x_std_ = (x - x_mean) / x_std

    # Standardize Y
    y_mean = y.mean()
    y_std = y.std() if y.std() != 0 else 1e-8
    y_std_ = (y - y_mean) / y_std

    N = x.shape[0]
    # w = (X^T y) / N
    w = (x_std_.T @ y_std_) / N

    return w

def display_binary_feature_importance(X, y, title):
    """
    Display feature importance for binary classification.
    """
    # Create dictionary to store feature names and their importance values
    feature_importance = {}

    # Iterate through each feature
    for x in X.columns:
        x_np = X[x].to_numpy(dtype=float)
        w = compute_binary_feature_importance(x_np, y, x)

        # Store the feature name and its importance value
        feature_importance[x] = w

    # Sort features by importance (descending by absolute value for visualization)
    sorted_features = sorted(feature_importance.items(), key=lambda item: abs(item[1]), reverse=True)

    # Get feature names and importance values for plotting
    features = [item[0] for item in sorted_features]
    importance_values = [item[1] for item in sorted_features]

    # Create horizontal bar chart with improved spacing
    plt.figure(figsize=(14, 10))  # Increased figure size

    # Add more left margin for feature labels
    plt.subplots_adjust(left=0.25, right=0.9)

    bars = plt.barh(features, importance_values,
                   color=['skyblue' if x > 0 else 'lightcoral' for x in importance_values])

    # Add labels and title
    plt.xlabel('Regression Coefficient', fontsize=12)
    plt.ylabel('Features', fontsize=12)
    plt.title(title, fontsize=14)
    plt.grid(axis='x', linestyle='--', alpha=0.7)

    # Add the importance values with better positioning
    for i, bar in enumerate(bars):
        width = bar.get_width()
        # Adjust text position based on bar width
        if width > 0:
            label_pos = width + 0.02  # More space for positive values
            ha = 'left'
        else:
            label_pos = width - 0.02  # More space for negative values
            ha = 'right'

        plt.text(label_pos, bar.get_y() + bar.get_height()/2, f'{width:.3f}',
                 va='center', ha=ha, fontsize=10)

    # Add more padding to x-axis limits
    plt.xlim(min(importance_values) * 1.15, max(importance_values) * 1.15)

    plt.tight_layout()
    plt.show()

    return feature_importance

response_vector = pd.factorize(breast_df['target'])[0]
input_matrix = breast_df.drop(columns=['target'])
binary_importance = display_binary_feature_importance(input_matrix, response_vector,
                                             "Feature Importance for Breast Cancer Dataset")

"""### 3. Feature importance multi-class classification"""

# Section 3: Multi-Class Feature Importance
def compute_multiclass_feature_importance(X, y_encoded):
    """
    Compute feature importance for multi-class classification using simple regression.
    1) Standardize X and y
    2) Compute W = X^T Y/N ∈ RD×C for multi-class

    Parameters:
    X: feature matrix (standardized)
    y_encoded: one-hot encoded target matrix (standardized)

    Returns:
    W: coefficient matrix of shape (num_features, num_classes)
    """
    # Number of samples
    N = X.shape[0]

    # Compute W = X^T Y/N
    W = (X.T @ y_encoded) / N

    return W

def one_hot_encode(y, num_classes):
    """
    Convert class labels to one-hot encoding
    """
    y_encoded = np.zeros((len(y), num_classes))
    for i in range(len(y)):
        y_encoded[i, y[i]] = 1
    return y_encoded

def standardize_matrix(X):
    """
    Standardize a matrix by columns
    """
    X_mean = X.mean(axis=0)
    X_std = X.std(axis=0)

    # Check if X_std is one value
    if np.isscalar(X_std):
        # If it's a scalar and 0, replace with a small value
        if X_std == 0:
            X_std = 1e-8
    else:
        # If it's an array
        X_std[X_std == 0] = 1e-8  #division by zero

    return (X - X_mean) / X_std

def display_multiclass_feature_importance(X, y, title, class_names=None):
    """
    Display feature importance for multi-class classification.
    Creates one plot per class showing the importance of each feature.
    """
    # Get number of classes
    num_classes = len(np.unique(y))

    # Convert category indices to one-hot encoding
    y_encoded = one_hot_encode(y, num_classes)

    # Standardize features and targets
    X_std = standardize_matrix(X.to_numpy(dtype=float))
    y_encoded_std = standardize_matrix(y_encoded)

    # Compute feature importance
    W = compute_multiclass_feature_importance(X_std, y_encoded_std)

    # Set default class names if not provided
    if class_names is None:
        class_names = [f"Class {i}" for i in range(num_classes)]

    # Create one plot per class
    for c in range(num_classes):
        # Get importance values for this class
        class_importance = W[:, c]

        # Create a dictionary of feature importance for this class
        feature_dict = dict(zip(X.columns, class_importance))

        # Sort features by absolute importance
        sorted_features = sorted(feature_dict.items(), key=lambda item: abs(item[1]), reverse=True)

        # Get feature names and importance values for plotting
        features = [item[0] for item in sorted_features]
        importance_values = [item[1] for item in sorted_features]

        # Create horizontal bar chart with improved spacing
        plt.figure(figsize=(14, 10))  # Increased figure size

        # Add more left margin for feature labels
        plt.subplots_adjust(left=0.25, right=0.9)

        bars = plt.barh(features, importance_values,
                       color=['skyblue' if x > 0 else 'lightcoral' for x in importance_values])

        # Add labels and title
        plt.xlabel('Regression Coefficient', fontsize=12)
        plt.ylabel('Features', fontsize=12)
        plt.title(f"{title} - {class_names[c]}", fontsize=14)
        plt.grid(axis='x', linestyle='--', alpha=0.7)

        # Add the importance values with better positioning
        for i, bar in enumerate(bars):
            width = bar.get_width()
            # Adjust text position based on bar width
            if width > 0:
                label_pos = width + 0.02  # More space for positive values
                ha = 'left'
            else:
                label_pos = width - 0.02  # More space for negative values
                ha = 'right'

            plt.text(label_pos, bar.get_y() + bar.get_height()/2, f'{width:.3f}',
                     va='center', ha=ha, fontsize=10)

        # Add more padding to x-axis limits
        plt.xlim(min(importance_values) * 1.15, max(importance_values) * 1.15)

        plt.tight_layout()
        plt.show()

    return W

response_vector = pd.factorize(penguin_df['species'])[0]
input_matrix = penguin_df.drop(columns=['species'])
# Get class names for better visualization
class_names = np.unique(penguin_df['species'])
multiclass_importance = display_multiclass_feature_importance(input_matrix, response_vector,
                                                   "Feature Importance for Penguin Dataset",
                                                   class_names)

"""### 4. Feature importance discussion

#### 4.1 Feature Importance Analysis for breast cancer dataset

The visualization for breast cancer data reveals clear patterns in feature importance.

The strongest negative features (around -0.7 to -0.8) are mostly related to cell boundary irregularity (`concave_points`) and cell size measurements (`radius`, `perimeter`, `area`). These strong negative values indicate larger, irregular cells are strongly associated with malignancy.

On the positive side, features like `smoothness2` and `symmetry2` have very weak positive coefficients (only around 0.01-0.06), suggesting minimal influence on benign classification.

This aligns with biological understanding, as cancer cells typically exhibit irregular shapes and borders. As shown in Wolberg et al. (1995), malignant cells display more boundary irregularities than benign ones. These results confirm these cell characteristics are the strongest predictors.

Features with suffixes (1, 2, 3) all appear in the results, suggesting different statistical measures of the same characteristics (possibly mean, standard error, and worst values) all contribute to prediction.

In conclusion, cell size and border irregularity are by far the most important features for breast cancer detection, matching established knowledge about cancer cell morphology.

#### 4.2 Feature Importance Analysis for Penguin Dataset

The feature importance visualizations for the three penguin species (Adelie, Chinstrap, and Gentoo) reveal distinct morphological characteristics that define each species.

For the Adelie penguins, culmen depth (bill thickness) has a strong positive correlation (0.539), while culmen length, flipper length, and body mass all show negative coefficients. This suggests Adelie penguins are characterized by thicker but shorter bills compared to other species.

Chinstrap penguins show a different pattern, with culmen length (0.449) and culmen depth (0.321) both having positive coefficients, while body mass (-0.292) and flipper length (-0.181) are negative. This indicates Chinstraps typically have longer, deeper bills but smaller body size and flipper length compared to average.

Gentoo penguins display the most distinctive profile with strong positive coefficients for flipper length (0.868) and body mass (0.818), suggesting they are significantly larger penguins overall. Their culmen length is moderately positive (0.493), while culmen depth is strongly negative (-0.824), indicating they have longer but relatively thin bills.

These patterns align with known morphological differences. According to studies on Antarctic penguins (Gorman et al., 2014), Gentoo penguins are indeed the largest of these three species, with longer flippers and larger body mass, while Adelie penguins typically have the shortest, deepest bills.

The feature importance analysis effectively captures the key physical distinctions between these species, demonstrating how simple regression coefficients can highlight the most important morphological features for species classification.

### 5. Remove unnecessary features

#### Feature Selection Analysis

**Penguin Dataset:**
After careful analysis of the feature importance visualizations for all three penguin species, it's evident that each of the four morphological features provides significant discriminative power. The culmen measurements, flipper length, and body mass all show substantial coefficient values that vary meaningfully across Adelie, Chinstrap, and Gentoo penguins. These variations align with the known physical differences between the species, confirming their predictive value. While some features like culmen depth are particularly important for distinguishing Adelie penguins, others like flipper length are critical for identifying Gentoo penguins. It's worth noting that the 'island' and 'sex' features have been removed from the dataset to focus exclusively on morphological characteristics, eliminating potential geographical or gender-based biases from the classification process.

**Breast Cancer Dataset:**
The breast cancer feature importance analysis reveals a spectrum of predictive power across the cellular measurements. The features related to cell boundary irregularity (concave points, concavity) and size measurements (radius, area, perimeter) demonstrate the strongest associations with malignancy classification, displaying coefficient values between -0.5 and -0.8. These strong negative correlations indicate that increases in these characteristics substantially decrease the likelihood of benign classification, which aligns with medical understanding of cancer cell morphology. Though some features like texture and symmetry measures show comparatively smaller coefficients, they still contribute meaningful information to the overall classification task. Even these weaker predictors exceed the minimum threshold of significance (0.05), suggesting they capture subtle but relevant characteristics of the cell samples. Preserving all features ensures the model has access to both the primary and secondary indicators of malignancy, allowing for more robust diagnostic capabilities.

# Task 2

As baseline models, implement multiple linear regression on the two classification tasks. Similar to the
simple regression, you may standardize the binary and the one-hot-encoded class labels and treat them
as continuous response variables. This enables us to train multiple linear regression y = Xw+ϵ for binary
classification and multivariate and multiple linear regression for multi-class classification Y = XW+ E by
minimizing the sum of squared errors (SSE). You may implement closed-form solutions for both models
(i.e., w = (X⊤X)
−1X⊤y for univariate and W = (X⊤X)
−1X⊤Y for multivariate regression), coordinate
descent, or gradient descent to fit the two models

### Define Linear Regression class and functions
"""

class LinearRegression:
    def __init__(self, add_bias=True):
        self.add_bias = add_bias
        pass

    def fit(self, x, y):
        if x.ndim == 1:
            x = x[:, None]                         #add a dimension for the features
        N = x.shape[0]
        if self.add_bias:
            x = np.column_stack([x,np.ones(N)])    #add bias by adding a constant feature of value 1
        #alternatively: self.w = np.linalg.inv(x.T @ x)@x.T@y
        self.w = np.linalg.lstsq(x, y)[0]          #return w for the least square difference
        return self

    def predict(self, x):
        N = x.shape[0]
        if self.add_bias:
            x = np.column_stack([x,np.ones(N)])
        yh = x@self.w                             #predict the y values
        return yh

def sigmoid(z):
  '''
  applies sigmoid function to get probabilities for binary classification
  input: continous value z
  output: probability
  '''
  return 1 / (1 + np.exp(-z))

def softmax(Z):
    """
    Numerically stable softmax.
    Z shape: (N, K)
    returns: (N, K)
    """

    if np.isnan(Z).any():
        raise ValueError("Input contains NaN values. Please clean the input data.")

    Z_shifted = Z - np.max(Z, axis=1, keepdims=True)
    expZ = np.exp(Z_shifted)

    denominator = np.sum(expZ, axis=1, keepdims=True)

    if np.any(denominator == 0):
        raise ValueError("Denominator is zero.")

    output = expZ / np.sum(expZ, axis=1, keepdims=True)

    if np.isnan(output).any():
        raise ValueError("Softmax output contains NaN values. Check the input data.")

    return output

"""### Applying linear regression"""

"""
1) Split data 80/20
2) Standardize features only
3) Apply LinearRegression + sigmoid (binary) or softmax (multiclass)
4) Evaluate results
"""

# -------------------------
# 1. TRAIN-TEST SPLIT
# -------------------------
breast_features_train, breast_features_test, breast_target_train, breast_target_test = train_test_split(
    breast_df.drop(columns=['target']),
    pd.factorize(breast_df['target'])[0],
    test_size=0.2,
    random_state=42
)

penguin_features_train, penguin_features_test, penguin_target_train, penguin_target_test = train_test_split(
    penguin_df.drop(columns=['species']),
    pd.factorize(penguin_df['species'])[0],
    test_size=0.2,
    random_state=42
)

# -------------------------
# 2. STANDARDIZE FEATURES ONLY
# -------------------------
breast_features_train = standardize_matrix(breast_features_train.to_numpy(dtype=float))
breast_features_test = standardize_matrix(breast_features_test.to_numpy(dtype=float))

penguin_features_train = standardize_matrix(penguin_features_train.to_numpy(dtype=float))
penguin_features_test = standardize_matrix(penguin_features_test.to_numpy(dtype=float))

# One-hot encode penguin targets (multiclass), but DO NOT standardize the one-hot
penguin_target_train = one_hot_encode(penguin_target_train, 3)

# -------------------------
# 3. LINEAR REGRESSION + SIGMOID / SOFTMAX
# -------------------------
# Binary classification (Breast Cancer)
model1 = LinearRegression()
breast_target_predicted = sigmoid(
    model1.fit(breast_features_train, breast_target_train).predict(breast_features_test)
)
w_breast = model1.w  # Fitted weights

# Multiclass classification (Penguins)
model2 = LinearRegression()
penguin_target_predicted = softmax(
    model2.fit(penguin_features_train, penguin_target_train).predict(penguin_features_test)
)
w_penguin = model2.w  # Fitted weights

# -------------------------
# 4. EVALUATE RESULTS
# -------------------------
# BREAST (Binary) – Compute ROC and AUROC
fpr, tpr, thresholds = roc_curve(breast_target_test, breast_target_predicted)
roc_auc = roc_auc_score(breast_target_test, breast_target_predicted)

plt.clf()
plt.plot(fpr, tpr, "b-", lw=2, label="AUROC = %0.2f" % roc_auc)
plt.axline((0, 0), (1, 1), linestyle="--", lw=1, color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC in predicting cancer')
plt.legend(loc="best")
plt.savefig("roc_curve.png", bbox_inches='tight', dpi=300)
print(f"Breast Cancer AUROC: {roc_auc:.3f}")

# PENGUIN (Multiclass) – Convert softmax probability matrix to class labels
penguin_predicted_class = np.argmax(penguin_target_predicted, axis=1)
penguin_accuracy = accuracy_score(penguin_target_test, penguin_predicted_class)
print(f"Penguin Multiclass Accuracy: {penguin_accuracy:.3f}")

"""# Task 3: Multiclassification Models

## Gradient Checking Utiltity
"""

def _softmax(Z):
    Z_shifted = Z - np.max(Z, axis=1, keepdims=True)
    expZ = np.exp(Z_shifted)
    return expZ / np.sum(expZ, axis=1, keepdims=True)

def cross_entropy_loss_and_grad(X, Y_onehot, W):
    N, d = X.shape
    Z = X @ W
    Y_hat = _softmax(Z)
    loss = -np.sum(Y_onehot * np.log(Y_hat + 1e-15)) / N
    grad = (X.T @ (Y_hat - Y_onehot)) / N
    return loss, grad

def gradient_check(X, Y_onehot, W, eps=1e-7, verbose=True):
    _, grad_analytical = cross_entropy_loss_and_grad(X, Y_onehot, W)
    grad_numerical = np.zeros_like(W)

    for i in range(W.shape[0]):
        for j in range(W.shape[1]):
            original_val = W[i, j]

            W[i, j] = original_val + eps
            loss_plus, _ = cross_entropy_loss_and_grad(X, Y_onehot, W)

            W[i, j] = original_val - eps
            loss_minus, _ = cross_entropy_loss_and_grad(X, Y_onehot, W)

            grad_numerical[i, j] = (loss_plus - loss_minus) / (2.0 * eps)
            W[i, j] = original_val

    diff = np.linalg.norm(grad_analytical - grad_numerical)
    denom = np.linalg.norm(grad_analytical) + np.linalg.norm(grad_numerical)
    rel_error = diff / (denom + 1e-15)

    if verbose:
        print("Gradient Check:")
        print(" > Absolute difference: {:.8e}".format(diff))
        print(" > Relative difference: {:.8e}".format(rel_error))

    return rel_error

"""## Classification Models

### Linear Regression Multi Classifier with Gradient descent
"""

class LinearRegressionMultiClassifierGD:
    def __init__(self, lr=0.01, max_iter=200):
        self.lr = lr
        self.max_iter = max_iter
        self.W = None
        self.loss_history_ = []
        self.acc_history_ = []
        self.best_iteration_ = None
        self.best_accuracy_ = -1
        self.best_W_ = None

    def fit(self, X, Y_onehot):
        N, d = X.shape
        K = Y_onehot.shape[1]
        self.W = np.zeros((d, K))

        for it in range(self.max_iter):
            scores = X @ self.W
            residual = scores - Y_onehot
            loss = np.mean(residual**2)
            self.loss_history_.append(loss)

            y_pred_class = np.argmax(scores, axis=1)
            y_true_class = np.argmax(Y_onehot, axis=1)
            acc = np.mean(y_pred_class == y_true_class)
            self.acc_history_.append(acc)

            if acc > self.best_accuracy_:
                self.best_accuracy_ = acc
                self.best_iteration_ = it
                self.best_W_ = self.W.copy()

            grad = (2.0 / N) * (X.T @ residual)
            self.W -= self.lr * grad

        self.W = self.best_W_

    def predict(self, X):
        scores = X @ self.W
        return np.argmax(scores, axis=1)

    def plot_training_curves(self):
        plt.figure(figsize=(8,4))
        plt.plot(self.loss_history_, label="Training MSE Loss")
        plt.axvline(self.best_iteration_, color='r', linestyle='--', label="Best iteration")
        plt.title("Multi-class Linear (MSE): Loss vs. Iterations")
        plt.xlabel("Iteration")
        plt.ylabel("MSE Loss")
        plt.legend()
        plt.show()

        plt.figure(figsize=(8,4))
        plt.plot(self.acc_history_, color="green", label="Training Accuracy")
        plt.axvline(self.best_iteration_, color='r', linestyle='--', label="Best iteration")
        plt.title("Multi-class Linear (MSE): Accuracy vs. Iterations")
        plt.xlabel("Iteration")
        plt.ylabel("Accuracy")
        plt.legend()
        plt.show()

"""### Linear Regression Multi Classifier OLS solution and Softmax Regression"""

class LinearRegressionMultiClassifier:
    def __init__(self):
        self.W = None

    def fit_closed_form(self, X, Y):
        XTX = X.T @ X
        XTY = X.T @ Y
        self.W = np.linalg.inv(XTX) @ XTY

    def predict(self, X):
        scores = X @ self.W
        return np.argmax(scores, axis=1)

"""### Softmax Regression"""

class SoftmaxRegression:
    def __init__(self, lr=0.01, max_iter=1000):
        self.lr = lr
        self.max_iter = max_iter
        self.W = None
        self.loss_history_ = []
        self.best_iteration_ = None
        self.best_val_acc_ = -1
        self.best_W_ = None

    def fit(self, X, Y_onehot):
        N, d = X.shape
        _, K = Y_onehot.shape
        self.W = np.zeros((d, K))

        for i in range(self.max_iter):
            loss, grad = cross_entropy_loss_and_grad(X, Y_onehot, self.W)
            self.loss_history_.append(loss)
            self.W -= self.lr * grad

    def fit_with_validation(self, X_train, Y_train, X_val, y_val, K):
        N_train, d = X_train.shape
        self.W = np.zeros((d, K))

        for i in range(self.max_iter):
            loss, grad = cross_entropy_loss_and_grad(X_train, Y_train, self.W)
            self.loss_history_.append(loss)
            self.W -= self.lr * grad

            y_val_pred = self.predict(X_val)
            val_acc = (y_val_pred == y_val).mean()

            if val_acc > self.best_val_acc_:
                self.best_val_acc_ = val_acc
                self.best_iteration_ = i
                self.best_W_ = self.W.copy()

        self.W = self.best_W_

    def predict(self, X):
        Z = X @ self.W
        Y_prob = _softmax(Z)
        return np.argmax(Y_prob, axis=1)

"""### Binary Logistic classification"""

breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17)
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 100)

breast_df = breast_cancer_wisconsin_diagnostic.data.features
breast_df['target'] = breast_cancer_wisconsin_diagnostic.data.targets
breast_df = breast_df.dropna()

print(f"[Breast Cancer] Data shape after loading: {breast_df.shape}")
y_breast = pd.factorize(breast_df['target'])[0]
X_df_breast = breast_df.drop(columns=['target'])
X_np_breast = X_df_breast.to_numpy().astype(float)

scaler_breast = StandardScaler()
X_bal_breast = scaler_breast.fit_transform(X_np_breast)

# (B) TRAIN/VAL/TEST SPLIT
X_train_b, X_temp_b, y_train_b, y_temp_b = train_test_split(
    X_bal_breast, y_breast, test_size=0.4, random_state=42, stratify=y_breast
)
X_val_b, X_test_b, y_val_b, y_test_b = train_test_split(
    X_temp_b, y_temp_b, test_size=0.5, random_state=42, stratify=y_temp_b
)

K_breast = len(np.unique(y_breast))
Y_train_onehot_b = np.eye(K_breast)[y_train_b]

logistic_model_b = LogisticRegressionBinary(lr=0.01, max_iter=300)
logistic_model_b.fit_with_validation(X_train_b, y_train_b, X_val_b, y_val_b, K_breast)

print("\n[Breast Cancer] Best iteration found:", logistic_model_b.best_iteration_)
print("[Breast Cancer] Best validation accuracy:", logistic_model_b.best_val_acc_)

y_test_pred_b = logistic_model_b.predict(X_test_b)
test_acc_b = (y_test_pred_b == y_test_b).mean()
print("[Breast Cancer] Final Test Accuracy (best iteration):", test_acc_b)

plt.figure()
plt.plot(logistic_model_b.loss_history_, label="Training Cross-Entropy Loss")
plt.axvline(x=logistic_model_b.best_iteration_, color='r', linestyle='--',
            label="Best iteration")
plt.title("[Breast Cancer] Logistic Regression: Training Loss vs. Iterations")
plt.xlabel("Iteration")
plt.ylabel("Loss")
plt.legend()
plt.show()

# ---- ROC for Breast Cancer (binary) ----
plot_binary_roc_auc(X_test_b, y_test_b, logistic_model_b, dataset_name="Breast Cancer")

print("----------------------------------------------------------------")
print("ROC CURVE LINEAR REGRESSION + LOGISTIC REGRESSION")
print("-------------------------------------------------------------")

# Making sure we are using same tests
kX_train_b, kX_test_b, ky_train_b, ky_test_b = train_test_split(
    X_bal_breast, y_breast, test_size=0.2, random_state=42, stratify=y_breast
)

# Logistic regression plotting
# Use the logistic_model_b that was already trained and fitted
# Since LogisticRegressionBinary doesn't have predict_proba, we'll implement it manually
logits = kX_test_b @ logistic_model_b.W
logistic_probs = 1 / (1 + np.exp(-logits))  # sigmoid function
log_fpr, log_tpr, _ = roc_curve(ky_test_b, logistic_probs)
log_roc_auc_val = auc(log_fpr, log_tpr)

# Linear regression plotting
model1 = LinearRegression()
model1.fit(kX_train_b, ky_train_b)
linear_regression_breast_predicted = sigmoid(model1.predict(kX_test_b))

lin_fpr, lin_tpr, _ = roc_curve(ky_test_b, linear_regression_breast_predicted)
lin_roc_auc_val = auc(lin_fpr, lin_tpr)

# Plot both
plt.figure(figsize=(10, 8))
plt.plot([0, 1], [0, 1], 'k--', label="Chance")
plt.plot(log_fpr, log_tpr, label=f"Logistic Regression (AUC: {log_roc_auc_val:.4f})")
plt.plot(lin_fpr, lin_tpr, label=f"Linear Regression (AUC: {lin_roc_auc_val:.4f})")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Breast Cancer ROC Curve: Linear Regression vs Logistic Regression")
plt.legend(loc="lower right")
plt.grid(True, alpha=0.3)
plt.show()

print(f"Breast Cancer Logistic Regression AUROC: {log_roc_auc_val:.4f}")
print(f"Breast Cancer Linear Regression AUROC: {lin_roc_auc_val:.4f}")
print("---------------------------------------------------------------")

# (C) Gradient Check on Breast subset
X_sub_b, y_sub_b = get_balanced_subset(X_bal_breast, y_breast, samples_per_class=5, random_seed=123)
K_sub_b = len(np.unique(y_sub_b))
if K_sub_b < 2:
    print("\n[Breast Cancer] Not enough distinct classes for gradient check.")
else:
    Y_sub_onehot_b = np.eye(K_sub_b)[y_sub_b]
    d_b = X_sub_b.shape[1]
    W_rand_b = np.random.randn(d_b, K_sub_b) * 0.01
    print("\n[Breast Cancer] Performing Gradient Check on Softmax Regression subset:")
    rel_error_b = gradient_check(X_sub_b, Y_sub_onehot_b, W_rand_b)
    print("[Breast Cancer] Relative Error (should be ~1e-6 or smaller):", rel_error_b)
    d_b = X_sub_b.shape[1]
    # For binary logistic regression, W is shape (d,)
    W_rand_b = np.random.randn(d_b) * 0.01

    print("\n[Breast Cancer] Performing Gradient Check on Binary Logistic Regression subset:")
    rel_error_b = gradient_check_binary(X_sub_b, y_sub_b, W_rand_b)
    print("[Breast Cancer] Relative Error (should be ~1e-6 or smaller):", rel_error_b)

# (D) MSE-based iterative model
Y_train_onehot_mse_b = np.eye(K_breast)[y_train_b]
mse_model_b = LinearRegressionMultiClassifierGD(lr=0.01, max_iter=300)
print("\n=== [Breast Cancer] Training MSE-based Multi-Class Model (Iterative) ===")
mse_model_b.fit(X_train_b, Y_train_onehot_mse_b)
print(f"[Breast Cancer] Best iteration: {mse_model_b.best_iteration_}, Best Acc = {mse_model_b.best_accuracy_:.4f}")
mse_model_b.plot_training_curves()

y_test_pred_mse_b = mse_model_b.predict(X_test_b)
test_acc_mse_b = (y_test_pred_mse_b == y_test_b).mean()
print("[Breast Cancer] Test Accuracy (MSE-based linear classifier):", test_acc_mse_b)

"""### K-Fold Cross Validation"""

def cross_val_accuracy(model, X, y, k=5, multi_class=False):
    kf = KFold(n_splits=k, shuffle=True, random_state=123)
    accs = []
    for train_idx, test_idx in kf.split(X):
        X_tr, X_te = X[train_idx], X[test_idx]
        y_tr, y_te = y[train_idx], y[test_idx]

        if multi_class:
            y_tr = y_tr.astype(int)
            K_ = len(np.unique(y))
            y_tr_onehot = np.eye(K_)[y_tr]

            if hasattr(model, 'fit_closed_form'):
                model.fit_closed_form(X_tr, y_tr_onehot)
            else:
                model.fit(X_tr, y_tr_onehot)

            y_pred = model.predict(X_te)
        else:
            if hasattr(model, 'fit_closed_form'):
                model.fit_closed_form(X_tr, y_tr)
            else:
                model.fit(X_tr, y_tr)

            y_pred = model.predict(X_te)

        acc = (y_pred == y_te).mean()
        accs.append(acc)

    return np.mean(accs), np.std(accs)

"""### Data Loading & Preprocessing"""

def basic_preprocess_penguins(df):
    species_map = {'Adelie': 0, 'Gentoo': 1, 'Chinstrap': 2}
    df['species'] = df['species'].map(species_map)
    y = df['species'].values.astype(int)
    X_df = df.drop(columns=['species'])
    X_df = pd.get_dummies(X_df, drop_first=True)
    return X_df, y

def load_penguins_data(file_path="drive/MyDrive/ML_A2/penguins_size.csv"):
    df = pd.read_csv(file_path)
    df.drop(columns=['island', 'sex'], errors='ignore', inplace=True)
    df.dropna(inplace=True)
    return df

def get_balanced_subset(X, y, samples_per_class=5, random_seed=42):
    np.random.seed(random_seed)
    classes = np.unique(y)

    X_subset_list = []
    y_subset_list = []

    for c in classes:
        idx_c = np.where(y == c)[0]
        if len(idx_c) <= samples_per_class:
            chosen_idx = idx_c
        else:
            chosen_idx = np.random.choice(idx_c, size=samples_per_class, replace=False)
        X_subset_list.append(X[chosen_idx])
        y_subset_list.append(y[chosen_idx])

    X_subset = np.vstack(X_subset_list)
    y_subset = np.hstack(y_subset_list)
    return X_subset, y_subset

"""### ROC / AUROC Helpers"""

def plot_multiclass_roc_auc(X_test, y_test, model, dataset_name="Penguins"):
    """
    Plots One-vs-Rest ROC curves + micro-average for multi-class data.
    For a Softmax model, we can get probability as softmax(X_test @ W).
    """
    # 1) Probability from softmax
    Z_test = X_test @ model.W
    expZ = np.exp(Z_test - np.max(Z_test, axis=1, keepdims=True))
    Y_prob = expZ / np.sum(expZ, axis=1, keepdims=True)

    n_classes = len(np.unique(y_test))
    # Binarize y_test for ROC
    y_test_bin = label_binarize(y_test, classes=range(n_classes))

    # 2) Compute ROC for each class (One-vs-Rest)
    fpr = {}
    tpr = {}
    roc_auc = {}
    for i in range(n_classes):
        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], Y_prob[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])

    # micro-average
    fpr["micro"], tpr["micro"], _ = roc_curve(y_test_bin.ravel(), Y_prob.ravel())
    roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

    # 3) Plot
    plt.figure()
    plt.plot([0, 1], [0, 1], 'k--', label="Chance")
    for i in range(n_classes):
        plt.plot(fpr[i], tpr[i],
                 label=f"Class {i} (AUC = {roc_auc[i]:.2f})")
    plt.plot(fpr["micro"], tpr["micro"],
             label=f"micro-average (AUC = {roc_auc['micro']:.2f})",
             linestyle=':')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title(f"[{dataset_name}] ROC (One-vs-Rest)")
    plt.legend()
    plt.show()

def plot_binary_roc_auc(X_test, y_test, model, dataset_name="Breast Cancer"):
    """
    For binary classification. Handles both cases where:
    1. model.W is a vector (d,)
    2. model.W is a matrix (d, 1)
    3. model.W is a matrix (d, 2)
    """
    # Check the shape of model.W
    if len(model.W.shape) == 1 or (len(model.W.shape) == 2 and model.W.shape[1] == 1):
        # Case 1 or 2: For models with W shape (d,) or (d,1)
        # Just calculate probability directly using sigmoid
        z = X_test @ (model.W.reshape(-1)) # Ensure W is a vector
        pos_probs = 1 / (1 + np.exp(-z))
    else:
        # Case 3: For softmax-based models with W shape (d, 2)
        Z_test = X_test @ model.W
        # Safe softmax calculation
        max_Z = np.max(Z_test, axis=1, keepdims=True)
        expZ = np.exp(Z_test - max_Z)
        Y_prob = expZ / np.sum(expZ, axis=1, keepdims=True)
        # Probability of class 1
        pos_probs = Y_prob[:, 1]

    fpr, tpr, _ = roc_curve(y_test, pos_probs)
    roc_auc_val = auc(fpr, tpr)

    plt.figure()
    plt.plot([0, 1], [0, 1], 'k--', label="Chance")
    plt.plot(fpr, tpr, label=f"AUC = {roc_auc_val:.2f}")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title(f"[{dataset_name}] ROC Curve")
    plt.legend()
    plt.show()

    print(f"[{dataset_name}] AUROC: {roc_auc_val:.4f}")

"""## Model training

### Breast Cancer Dataset
"""

breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17)
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 100)

breast_df = breast_cancer_wisconsin_diagnostic.data.features
breast_df['target'] = breast_cancer_wisconsin_diagnostic.data.targets
breast_df = breast_df.dropna()

print(f"[Breast Cancer] Data shape after loading: {breast_df.shape}")
y_breast = pd.factorize(breast_df['target'])[0]
X_df_breast = breast_df.drop(columns=['target'])
X_np_breast = X_df_breast.to_numpy().astype(float)

scaler_breast = StandardScaler()
X_bal_breast = scaler_breast.fit_transform(X_np_breast)

# (B) TRAIN/VAL/TEST SPLIT
X_train_b, X_temp_b, y_train_b, y_temp_b = train_test_split(
    X_bal_breast, y_breast, test_size=0.4, random_state=42, stratify=y_breast
)
X_val_b, X_test_b, y_val_b, y_test_b = train_test_split(
    X_temp_b, y_temp_b, test_size=0.5, random_state=42, stratify=y_temp_b
)

K_breast = len(np.unique(y_breast))
Y_train_onehot_b = np.eye(K_breast)[y_train_b]

logistic_model_b = LogisticRegressionBinary(lr=0.01, max_iter=300)
logistic_model_b.fit_with_validation(X_train_b, y_train_b, X_val_b, y_val_b, K_breast)

print("\n[Breast Cancer] Best iteration found:", logistic_model_b.best_iteration_)
print("[Breast Cancer] Best validation accuracy:", logistic_model_b.best_val_acc_)

y_test_pred_b = logistic_model_b.predict(X_test_b)
test_acc_b = (y_test_pred_b == y_test_b).mean()
print("[Breast Cancer] Final Test Accuracy (best iteration):", test_acc_b)

plt.figure()
plt.plot(logistic_model_b.loss_history_, label="Training Cross-Entropy Loss")
plt.axvline(x=logistic_model_b.best_iteration_, color='r', linestyle='--',
            label="Best iteration")
plt.title("[Breast Cancer] Logistic Regression: Training Loss vs. Iterations")
plt.xlabel("Iteration")
plt.ylabel("Loss")
plt.legend()
plt.show()

# ---- ROC for Breast Cancer (binary) ----
plot_binary_roc_auc(X_test_b, y_test_b, logistic_model_b, dataset_name="Breast Cancer")

print("----------------------------------------------------------------")
print("ROC CURVE LINEAR REGRESSION + LOGISTIC REGRESSION")
print("-------------------------------------------------------------")

# Making sure we are using same tests
kX_train_b, kX_test_b, ky_train_b, ky_test_b = train_test_split(
    X_bal_breast, y_breast, test_size=0.2, random_state=42, stratify=y_breast
)

# Logistic regression plotting
# Use the logistic_model_b that was already trained and fitted
# Since LogisticRegressionBinary doesn't have predict_proba, we'll implement it manually
logits = kX_test_b @ logistic_model_b.W
logistic_probs = 1 / (1 + np.exp(-logits))  # sigmoid function
log_fpr, log_tpr, _ = roc_curve(ky_test_b, logistic_probs)
log_roc_auc_val = auc(log_fpr, log_tpr)

# Linear regression plotting
model1 = LinearRegression()
model1.fit(kX_train_b, ky_train_b)
linear_regression_breast_predicted = sigmoid(model1.predict(kX_test_b))

lin_fpr, lin_tpr, _ = roc_curve(ky_test_b, linear_regression_breast_predicted)
lin_roc_auc_val = auc(lin_fpr, lin_tpr)

# Plot both
plt.figure(figsize=(10, 8))
plt.plot([0, 1], [0, 1], 'k--', label="Chance")
plt.plot(log_fpr, log_tpr, label=f"Logistic Regression (AUC: {log_roc_auc_val:.4f})")
plt.plot(lin_fpr, lin_tpr, label=f"Linear Regression (AUC: {lin_roc_auc_val:.4f})")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Breast Cancer ROC Curve: Linear Regression vs Logistic Regression")
plt.legend(loc="lower right")
plt.grid(True, alpha=0.3)
plt.show()

print(f"Breast Cancer Logistic Regression AUROC: {log_roc_auc_val:.4f}")
print(f"Breast Cancer Linear Regression AUROC: {lin_roc_auc_val:.4f}")
print("---------------------------------------------------------------")

# (C) Gradient Check on Breast subset
X_sub_b, y_sub_b = get_balanced_subset(X_bal_breast, y_breast, samples_per_class=5, random_seed=123)
K_sub_b = len(np.unique(y_sub_b))
if K_sub_b < 2:
    print("\n[Breast Cancer] Not enough distinct classes for gradient check.")
else:
    Y_sub_onehot_b = np.eye(K_sub_b)[y_sub_b]
    d_b = X_sub_b.shape[1]
    W_rand_b = np.random.randn(d_b, K_sub_b) * 0.01
    print("\n[Breast Cancer] Performing Gradient Check on Softmax Regression subset:")
    rel_error_b = gradient_check(X_sub_b, Y_sub_onehot_b, W_rand_b)
    print("[Breast Cancer] Relative Error (should be ~1e-6 or smaller):", rel_error_b)
    d_b = X_sub_b.shape[1]
    # For binary logistic regression, W is shape (d,)
    W_rand_b = np.random.randn(d_b) * 0.01

    print("\n[Breast Cancer] Performing Gradient Check on Binary Logistic Regression subset:")
    rel_error_b = gradient_check_binary(X_sub_b, y_sub_b, W_rand_b)
    print("[Breast Cancer] Relative Error (should be ~1e-6 or smaller):", rel_error_b)

# (D) MSE-based iterative model
Y_train_onehot_mse_b = np.eye(K_breast)[y_train_b]
mse_model_b = LinearRegressionMultiClassifierGD(lr=0.01, max_iter=300)
print("\n=== [Breast Cancer] Training MSE-based Multi-Class Model (Iterative) ===")
mse_model_b.fit(X_train_b, Y_train_onehot_mse_b)
print(f"[Breast Cancer] Best iteration: {mse_model_b.best_iteration_}, Best Acc = {mse_model_b.best_accuracy_:.4f}")
mse_model_b.plot_training_curves()

y_test_pred_mse_b = mse_model_b.predict(X_test_b)
test_acc_mse_b = (y_test_pred_mse_b == y_test_b).mean()
print("[Breast Cancer] Test Accuracy (MSE-based linear classifier):", test_acc_mse_b)

"""### Penguin dataset"""

# 1) Penguins
df_penguins = load_penguins_data("drive/MyDrive/ML_A2/penguins_size.csv")
X_df_peng, y_peng = basic_preprocess_penguins(df_penguins)
print(f"[Penguins] Data shape after loading: {df_penguins.shape}")

X_np_peng = X_df_peng.to_numpy().astype(float)
scaler = StandardScaler()
X_bal_peng = scaler.fit_transform(X_np_peng)

models = {
    "Linear Multi (Closed-form)": LinearRegressionMultiClassifier(),
    "Softmax Multi": SoftmaxRegression(lr=0.01, max_iter=100),
}
for model_name, model in models.items():
    mc_flag = "Multi" in model_name
    mean_acc, std_acc = cross_val_accuracy(model, X_bal_peng, y_peng, k=5, multi_class=mc_flag)
    print(f"\n=== [Penguins] {model_name} (CV) ===")
    print(f"Accuracy (5-fold CV): {mean_acc:.4f} ± {std_acc:.4f}")

# (B) TRAIN/VAL/TEST for Penguins
X_train_p, X_temp_p, y_train_p, y_temp_p = train_test_split(
    X_bal_peng, y_peng, test_size=0.4, random_state=42, stratify=y_peng
)
X_val_p, X_test_p, y_val_p, y_test_p = train_test_split(
    X_temp_p, y_temp_p, test_size=0.5, random_state=42, stratify=y_temp_p
)

K_p = len(np.unique(y_peng))
Y_train_onehot_p = np.eye(K_p)[y_train_p]

softmax_model_p = SoftmaxRegression(lr=0.01, max_iter=300)
softmax_model_p.fit_with_validation(X_train_p, Y_train_onehot_p, X_val_p, y_val_p, K_p)

print("\n[Penguins] Best iteration found:", softmax_model_p.best_iteration_)
print("[Penguins] Best validation accuracy:", softmax_model_p.best_val_acc_)

y_test_pred_p = softmax_model_p.predict(X_test_p)
test_acc_p = (y_test_pred_p == y_test_p).mean()
print("[Penguins] Final Test Accuracy (best iteration):", test_acc_p)

# Plot training loss
plt.figure()
plt.plot(softmax_model_p.loss_history_, label="Training Cross-Entropy Loss")
plt.axvline(x=softmax_model_p.best_iteration_, color='r', linestyle='--',
            label="Best iteration")
plt.title("[Penguins] Softmax Regression: Training Loss vs. Iterations")
plt.xlabel("Iteration")
plt.ylabel("Loss")
plt.legend()
plt.show()

# ---- ROC for Penguins (multi-class) ----
plot_multiclass_roc_auc(X_test_p, y_test_p, softmax_model_p, dataset_name="Penguins")

# (C) Gradient Check on Penguins subset
X_sub_p, y_sub_p = get_balanced_subset(X_bal_peng, y_peng, samples_per_class=5, random_seed=123)
K_sub_p = len(np.unique(y_sub_p))
if K_sub_p < 2:
    print("\n[Penguins] Not enough distinct classes for gradient check.")
else:
    Y_sub_onehot_p = np.eye(K_sub_p)[y_sub_p]
    d_p = X_sub_p.shape[1]
    W_rand_p = np.random.randn(d_p, K_sub_p) * 0.01
    print("\n[Penguins] Performing Gradient Check on Softmax Regression subset:")
    rel_error_p = gradient_check(X_sub_p, Y_sub_onehot_p, W_rand_p)
    print("[Penguins] Relative Error (should be ~1e-6 or smaller):", rel_error_p)

# (D) MSE-based iterative model
Y_train_onehot_mse_p = np.eye(K_p)[y_train_p]
mse_model_p = LinearRegressionMultiClassifierGD(lr=0.01, max_iter=300)
print("\n=== [Penguins] Training MSE-based Multi-Class Model (Iterative) ===")
mse_model_p.fit(X_train_p, Y_train_onehot_mse_p)
print(f"[Penguins] Best iteration: {mse_model_p.best_iteration_}, Best Acc = {mse_model_p.best_accuracy_:.4f}")
mse_model_p.plot_training_curves()
y_test_pred_mse_p = mse_model_p.predict(X_test_p)
test_acc_mse_p = (y_test_pred_mse_p == y_test_p).mean()
print("[Penguins] Test Accuracy (MSE-based linear classifier):", test_acc_mse_p)

"""# Task 4: Experiments

## Binary Coefficient comparison
"""

def compare_multiple_feature_importance(importance_dict, title):
    """
    Display side-by-side comparison of feature importance from multiple models.

    Parameters:
    importance_dict: dictionary where keys are model names and values are dictionaries
                    of feature names and their importance
    title: title for the plot
    """
    if not importance_dict:
        raise ValueError("importance_dict cannot be empty")

    # Get model names
    model_names = list(importance_dict.keys())

    # Get all unique features across all models
    all_features = set()
    for model_name, importances in importance_dict.items():
        all_features.update(importances.keys())

    # Ensure all models have values for all features (use 0 if missing)
    for model_name in model_names:
        for feature in all_features:
            if feature not in importance_dict[model_name]:
                importance_dict[model_name][feature] = 0

    # Sort features by absolute importance value from the first model
    sort_by = model_names[0]
    sorted_features = sorted(
        all_features,
        key=lambda feature: abs(importance_dict[sort_by].get(feature, 0)),
        reverse=True
    )

    # Create a figure with subplots side by side
    num_models = len(model_names)
    # Adjust figure size for full width
    fig, axes = plt.subplots(1, num_models, figsize=(14, 10), sharey=True)

    # Handle case where there's only one model
    if num_models == 1:
        axes = [axes]

    # Plot each model
    for i, model_name in enumerate(model_names):
        # Get values for this model based on the sorted features
        model_values = [importance_dict[model_name][feature] for feature in sorted_features]

        # Plot on the corresponding subplot
        bars = axes[i].barh(sorted_features, model_values,
                        color=['skyblue' if x > 0 else 'lightcoral' for x in model_values])
        axes[i].set_xlabel(f'{model_name} Coefficient', fontsize=12)

        if i == 0:  # Only add y-label to the leftmost subplot
            axes[i].set_ylabel('Features', fontsize=12)

        axes[i].set_title(f'{model_name} Feature Importance', fontsize=14)
        axes[i].grid(axis='x', linestyle='--', alpha=0.7)

        # Add coefficient values
        for j, bar in enumerate(bars):
            width = bar.get_width()
            if width > 0:
                label_pos = width + abs(max(model_values)) * 0.02
                ha = 'left'
            else:
                label_pos = width - abs(min(model_values)) * 0.02
                ha = 'right'
            axes[i].text(label_pos, bar.get_y() + bar.get_height()/2, f'{width:.3f}',
                    va='center', ha=ha, fontsize=9)

        # Set appropriate x-axis limits with padding
        max_val = max(model_values) if max(model_values) > 0 else 0
        min_val = min(model_values) if min(model_values) < 0 else 0
        axes[i].set_xlim(min_val * 1.2, max_val * 1.2)

    plt.suptitle(title, fontsize=16, y=0.98)  # Adjust y position of main title

    # Apply tight layout with specific padding to prevent overlap
    plt.tight_layout(rect=[0, 0, 1, 0.93])

    # Add more space between subplots
    plt.subplots_adjust(wspace=0.2)

    return fig

# Example usage
# Convert model coefficients to dictionaries if needed
linear_importance_dict = dict(zip(breast_df.columns, w_breast))
simple_importance = dict(zip(breast_df.columns, binary_importance))
logistic_importance = dict(zip(breast_df.columns, logistic_model_b.W))




# Create dictionary of models
models = {
    "Simple Linear Regression": binary_importance,
    "Multiple Linear Regression": linear_importance_dict,
    "Logistic Regression": logistic_importance
    # Add additional models as needed
}

# Compare all models
fig = compare_multiple_feature_importance(
    models,
    "Comparison of Feature Importance Across Models"
)

# Display the plot
plt.show()

"""## Multiclass coefficient comparison"""

import matplotlib.pyplot as plt
import numpy as np

def plot_multiclass_coefficients(coefficients_list, model_names, feature_names, class_names, title):
    """
    Plot D × C heatmaps for multiple sets of multi-class model coefficients.

    Parameters:
    coefficients_list: list of coefficient arrays, each of shape (n_features, n_classes)
    model_names: list of model names corresponding to each coefficient array
    feature_names: list of feature names (D features)
    class_names: list of class names (C classes)
    title: main title for the plot
    """
    # Number of models to compare
    n_models = len(coefficients_list)

    # Create a figure with subplots
    fig, axes = plt.subplots(1, n_models, figsize=(14, 8), sharey=True)

    # Handle case where there's only one model
    if n_models == 1:
        axes = [axes]

    # Plot each model
    for i, (coefficients, model_name) in enumerate(zip(coefficients_list, model_names)):
        # Create a heatmap on the current subplot
        im = sns.heatmap(
            coefficients,
            annot=True,
            cmap='coolwarm',
            center=0,
            fmt='.3f',
            linewidths=.5,
            xticklabels=class_names,
            yticklabels=feature_names,
            cbar_kws={'label': 'Coefficient Value'},
            ax=axes[i]
        )

        # Set subplot title
        axes[i].set_title(f'{model_name}', fontsize=14)
        axes[i].set_xlabel('Penguin Class')

        if i == 0:
            axes[i].set_ylabel('Features')

    # Set main title with appropriate spacing
    plt.suptitle(title, fontsize=16, y=0.98)

    # Adjust layout to prevent overlap
    plt.tight_layout(rect=[0, 0, 1, 0.95])

    return fig

# Get feature names from the dataframe, excluding the 'species' column
features = [col for col in penguin_df.columns if col != 'species']

# Handle the extra feature in w_penguin
# Extract only the rows corresponding to the actual features (assuming extra feature is at the end)
w_penguin_adjusted = w_penguin[:4]  # Keep only the first 4 rows if there are 5 rows total

# Use the function with existing variables and adjusted data
fig = plot_multiclass_coefficients(
    coefficients_list=[multiclass_importance, w_penguin_adjusted, softmax_model.W],
    model_names=['Multi-class Simple Linear', 'Multi-class Linear Regression', 'Softmax Regression'],
    feature_names=features,
    class_names=['Adelie', 'Gentoo', 'Chinstrap'],
    title='Comparison of Penguin Classification Model Coefficients'
)

# Display the plot
plt.show()